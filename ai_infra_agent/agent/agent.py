import json
import asyncio
import datetime # Import datetime for timestamp
from typing import Dict, Any, List

from langchain.llms.base import BaseLLM
from langchain.schema import Generation, LLMResult
# A placeholder for a real LLM integration, e.g., from langchain.chat_models import ChatOpenAI
# For MVP, we might use a mock or a simple LLM wrapper.

from ai_infra_agent.state.manager import StateManager
from ai_infra_agent.infrastructure.tool_factory import ToolFactory
from ai_infra_agent.agent.prompt_builder import PromptBuilder
from ai_infra_agent.core.logging import logger

# Placeholder for a real LLM class
class SimpleMockLLM(BaseLLM):
    def _generate(self, prompts: List[str], stop: List[str] = None) -> LLMResult:
        # This mock will return a predefined plan for a specific request.
        # This is for MVP testing without a real LLM call.
        logger.info("Using Mock LLM. Returning a predefined plan from sample_data.json.")
        
        with open('sample_data.json', 'r') as f:
            data = json.load(f)

        response_text = json.dumps(data)
        
        generations = [[Generation(text=response_text)] for _ in prompts]

        # The _generate method must return an LLMResult object.
        return LLMResult(generations=generations)

    @property
    def _llm_type(self) -> str:
        return "simple_mock"


class StateAwareAgent:
    """
    The AI agent that understands the infrastructure state and processes user requests.
    """

    def __init__(self, settings, state_manager: StateManager, tool_factory: ToolFactory, logger, llm: BaseLLM = None):
        """
        Initializes the StateAwareAgent.

        Args:
            settings: The agent settings.
            state_manager (StateManager): The manager for infrastructure state.
            tool_factory (ToolFactory): The factory to create tools.
            logger: The logger instance.
            llm (BaseLLM, optional): The language model to use. Defaults to a mock LLM for MVP.
        """
        self.settings = settings
        self.state_manager = state_manager
        self.tool_factory = tool_factory
        self.llm = llm or SimpleMockLLM() # Use a mock LLM for now
        self.prompt_builder = PromptBuilder()
        self.logger = logger
        self.logger.info("StateAwareAgent initialized.")

    def _resolve_placeholders(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Resolves placeholders in the parameters dictionary.
        For MVP, only {{timestamp}} is resolved.
        """
        resolved_params = {}
        for key, value in params.items():
            if isinstance(value, str) and "{{timestamp}}" in value:
                timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
                resolved_params[key] = value.replace("{{timestamp}}", timestamp)
            else:
                resolved_params[key] = value
        return resolved_params

    async def process_request(self, request: str) -> Dict[str, Any]:
        """
        Processes a user request and generates an execution plan.

        Args:
            request (str): The user's request in natural language.

        Returns:
            Dict[str, Any]: The execution plan generated by the LLM.
        """
        logger.info(f"Processing request: '{request}'")

        # 1. Gather context
        current_state = self.state_manager.state.dict()
        # For MVP, we only care about EC2 tools.
        available_tools = self.tool_factory.get_tool_names()
        logger.debug(f"Available tools: {available_tools}")

        # 2. Build the prompt
        prompt = self.prompt_builder.build(
            state=current_state,
            tools=available_tools,
            request=request
        )
        logger.debug(f"Generated Prompt:\n{prompt}")

        # 3. Interact with the LLM
        try:
            logger.info("Sending prompt to LLM...")
            # Run the synchronous LLM call in a separate thread
            llm_response = await asyncio.to_thread(self.llm, prompt)
            logger.debug(f"Raw LLM response:\n{llm_response}") # Added debug log for raw response
            try:
                plan = json.loads(llm_response)
                logger.debug(f"Parsed LLM plan:\n{json.dumps(plan, indent=2)}") # Added debug log for parsed plan
            except json.JSONDecodeError as e:
                logger.error(f"Failed to decode LLM response into JSON: {e}")
                logger.error(f"Problematic LLM response content: {llm_response[:500]}...") # Log first 500 chars
                plan = {"error": "LLM returned invalid JSON.", "details": str(e)}
        except Exception as e:
            logger.error(f"An error occurred during LLM interaction: {e}")
            plan = {"error": str(e)}

        # 4. Return the plan (no complex validation in MVP)
        return plan

    async def execute_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """
        Executes a specific tool by name.
        """
        self.logger.info(f"Executing tool '{tool_name}' with params: {kwargs}")
        
        # Resolve placeholders in kwargs before executing the tool
        resolved_kwargs = self._resolve_placeholders(kwargs)
        self.logger.debug(f"Resolved tool parameters: {resolved_kwargs}")

        tool = self.tool_factory.create_tool(tool_name) # Corrected method call
        # The create_tool method already raises ValueError if not found, so no need for explicit check here.
        
        # The tool's execute method is synchronous, so we run it in an executor
        # to avoid blocking the asyncio event loop.
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, lambda: tool.execute(**resolved_kwargs))
        self.logger.info(f"Tool '{tool_name}' executed successfully.")
        return result